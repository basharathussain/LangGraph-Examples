{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Persistent Memory Agent - Complete Implementation\n",
    "\n",
    "This notebook demonstrates how to build an intelligent agent with persistent memory using LangGraph and FAISS.\n",
    "\n",
    "## üìã What We'll Build:\n",
    "- An agent that remembers conversations across sessions\n",
    "- Integration with web search and calculator tools\n",
    "- Intelligent query routing\n",
    "- Vector-based memory retrieval using FAISS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 1: Installation\n",
    "\n",
    "Run this cell to install all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Uncomment the line below if running for the first time\n",
    "# !pip install -U langchain langgraph langchain-openai duckduckgo-search faiss-cpu python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Step 2: Set Up API Keys\n",
    "\n",
    "You need an OpenAI API key for this tutorial. Get one at: https://platform.openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set your OpenAI API key\n",
    "# Option 1: Set it directly (not recommended for production)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# Option 2: Enter it securely when prompted\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "print(\"‚úÖ API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 3: Import Dependencies\n",
    "\n",
    "Import all necessary libraries for building our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain components for OpenAI integration\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Vector store for persistent memory\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Tools for extending agent capabilities\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.tools import tool\n",
    "\n",
    "# LangGraph for building the agent workflow\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, Any\n",
    "\n",
    "print(\"‚úÖ All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Step 4: Initialize Persistent Vector Store\n",
    "\n",
    "The vector store is the brain of our memory system. It stores conversation embeddings and enables similarity-based retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where our memory will be saved\n",
    "# This file will persist across sessions\n",
    "VECTOR_DB_PATH = \"chat_memory.faiss\"\n",
    "\n",
    "# Initialize OpenAI embeddings model\n",
    "# This converts text into 1536-dimensional vectors\n",
    "# We use 'text-embedding-3-small' for a good balance of quality and speed\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Check if we have existing memory from previous sessions\n",
    "if os.path.exists(VECTOR_DB_PATH):\n",
    "    # Load existing memory from disk\n",
    "    # allow_dangerous_deserialization=True is needed to load pickled data\n",
    "    vectorstore = FAISS.load_local(\n",
    "        VECTOR_DB_PATH, \n",
    "        embeddings, \n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    print(\"‚úÖ Loaded existing memory from disk\")\n",
    "    print(f\"üìä Current memory contains {vectorstore.index.ntotal} conversation entries\")\n",
    "else:\n",
    "    # Create a new vector store with an initialization message\n",
    "    vectorstore = FAISS.from_texts(\n",
    "        [\"Conversation initialized. This is the first session.\"], \n",
    "        embeddings\n",
    "    )\n",
    "    print(\"üÜï Created new memory store\")\n",
    "\n",
    "# The vectorstore is now ready to:\n",
    "# 1. Store new conversation embeddings\n",
    "# 2. Retrieve similar past conversations\n",
    "# 3. Persist to disk for future sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 5: Define Agent Tools\n",
    "\n",
    "Tools extend our agent's capabilities beyond text generation. We'll create a calculator and web search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DuckDuckGo search tool\n",
    "# This allows our agent to search the web for current information\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "# Create a custom calculator tool using the @tool decorator\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Evaluate a simple mathematical expression.\n",
    "    \n",
    "    Args:\n",
    "        expression: A math expression like '2+2', '10*5', or '100/4'\n",
    "    \n",
    "    Returns:\n",
    "        The result of the calculation or an error message\n",
    "    \n",
    "    Examples:\n",
    "        calculator('2+2') -> 'The result is 4'\n",
    "        calculator('10*5') -> 'The result is 50'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean the expression to only allow safe mathematical operations\n",
    "        # This prevents code injection attacks\n",
    "        safe_expr = re.sub(r'[^0-9+\\-*/.() ]', '', expression)\n",
    "        \n",
    "        # Evaluate the mathematical expression\n",
    "        # Note: In production, consider using a safer math parser\n",
    "        result = eval(safe_expr)\n",
    "        \n",
    "        return f\"The result is {result}\"\n",
    "    except Exception as e:\n",
    "        # Return a helpful error message if calculation fails\n",
    "        return f\"Error evaluating expression: {e}\"\n",
    "\n",
    "print(\"‚úÖ Tools initialized successfully\")\n",
    "print(\"üîç Available tools: Web Search, Calculator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 6: Initialize the Language Model\n",
    "\n",
    "We'll use GPT-4o-mini for generating responses and reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatOpenAI model\n",
    "# - model: gpt-4o-mini is fast and cost-effective\n",
    "# - temperature: 0 makes responses more deterministic and focused\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(\"‚úÖ Language model initialized\")\n",
    "print(\"üß† Using model: gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 7: Define Agent Nodes\n",
    "\n",
    "Each node represents a specialized processing step in our agent workflow.\n",
    "\n",
    "### 7.1 Controller Node (Query Router)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controller(state: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Route incoming queries to the appropriate specialized node.\n",
    "    \n",
    "    This is the \"brain\" of our routing system. It analyzes the user's query\n",
    "    and decides which specialized node should handle it.\n",
    "    \n",
    "    Args:\n",
    "        state: Dictionary containing the user's query\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'next' key indicating which node to route to\n",
    "    \n",
    "    Routing Logic:\n",
    "        - If query contains math operations ‚Üí route to 'calc'\n",
    "        - If query asks for information ‚Üí route to 'search'\n",
    "        - Otherwise ‚Üí route to 'reason' for general conversation\n",
    "    \"\"\"\n",
    "    # Extract the query and convert to lowercase for easier matching\n",
    "    query = state[\"query\"].lower()\n",
    "    \n",
    "    # Check if the query contains mathematical operations or keywords\n",
    "    math_keywords = [\"+\", \"-\", \"*\", \"/\", \"multiply\", \"divide\", \"calculate\", \"sum\", \"add\"]\n",
    "    if any(keyword in query for keyword in math_keywords):\n",
    "        print(\"üßÆ Controller: Routing to Calculator node\")\n",
    "        return {\"next\": \"calc\"}\n",
    "    \n",
    "    # Check if the query is asking for information that requires search\n",
    "    search_keywords = [\"who\", \"what\", \"when\", \"where\", \"news\", \"latest\", \"search\", \"find\", \"recent\"]\n",
    "    if any(keyword in query for keyword in search_keywords):\n",
    "        print(\"üîç Controller: Routing to Search node\")\n",
    "        return {\"next\": \"search\"}\n",
    "    \n",
    "    # Default to reasoning node for conversational queries\n",
    "    print(\"üí≠ Controller: Routing to Reasoning node\")\n",
    "    return {\"next\": \"reason\"}\n",
    "\n",
    "print(\"‚úÖ Controller node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Reasoning Node (Memory-Enhanced Conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoning(state: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Handle conversational queries with memory-enhanced context.\n",
    "    \n",
    "    This is the most sophisticated node. It:\n",
    "    1. Retrieves relevant past conversations from vector memory\n",
    "    2. Includes this context in the prompt to the LLM\n",
    "    3. Generates a contextually-aware response\n",
    "    4. Saves the new conversation to memory for future reference\n",
    "    \n",
    "    Args:\n",
    "        state: Dictionary containing the user's query\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'answer' key containing the response\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    print(f\"üí≠ Reasoning: Processing query: '{query}'\")\n",
    "    \n",
    "    # Step 1: Retrieve similar past conversations from memory\n",
    "    # k=3 means we get the 3 most similar past exchanges\n",
    "    print(\"   üîç Searching memory for relevant context...\")\n",
    "    retrieved_docs = vectorstore.similarity_search(query, k=3)\n",
    "    \n",
    "    # Extract the text content from retrieved documents\n",
    "    memory_context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    print(f\"   üìö Found {len(retrieved_docs)} relevant memories\")\n",
    "    \n",
    "    # Step 2: Build a prompt that includes the memory context\n",
    "    # This helps the LLM understand the conversation history\n",
    "    prompt = f\"\"\"You are a helpful AI assistant with memory of past conversations.\n",
    "\n",
    "Previous relevant context from our conversation history:\n",
    "{memory_context}\n",
    "\n",
    "Current user question: {query}\n",
    "\n",
    "Instructions:\n",
    "- Respond clearly and naturally\n",
    "- If the previous context is relevant, acknowledge and build upon it\n",
    "- If it's not relevant, answer the question directly\n",
    "- Be conversational and helpful\"\"\"\n",
    "    \n",
    "    # Step 3: Generate response using the LLM\n",
    "    print(\"   ü§ñ Generating response...\")\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Step 4: Save this exchange to memory for future reference\n",
    "    # Format: \"User: [question]\\nAssistant: [answer]\"\n",
    "    conversation_entry = f\"User: {query}\\nAssistant: {response.content}\"\n",
    "    vectorstore.add_texts([conversation_entry])\n",
    "    \n",
    "    # Step 5: Persist the updated memory to disk\n",
    "    vectorstore.save_local(VECTOR_DB_PATH)\n",
    "    print(\"   üíæ Memory updated and saved\")\n",
    "    \n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "print(\"‚úÖ Reasoning node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Calculator Node (Mathematical Operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(state: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Handle mathematical calculations.\n",
    "    \n",
    "    This node:\n",
    "    1. Extracts the mathematical expression from the query\n",
    "    2. Uses the calculator tool to compute the result\n",
    "    3. Saves the calculation to memory\n",
    "    \n",
    "    Args:\n",
    "        state: Dictionary containing the user's query\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'answer' key containing the calculation result\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    print(f\"üßÆ Calculator: Processing calculation: '{query}'\")\n",
    "    \n",
    "    # Extract only the mathematical expression from the query\n",
    "    # This removes text like \"what is\" or \"calculate\"\n",
    "    expr = re.sub(r\"[^0-9+\\-*/(). ]\", \"\", query)\n",
    "    print(f\"   üìä Extracted expression: '{expr}'\")\n",
    "    \n",
    "    # Use the calculator tool to compute the result\n",
    "    result = calculator.invoke(expr)\n",
    "    print(f\"   ‚úÖ Result: {result}\")\n",
    "    \n",
    "    # Save this calculation to memory\n",
    "    conversation_entry = f\"User: {query}\\nAssistant: {result}\"\n",
    "    vectorstore.add_texts([conversation_entry])\n",
    "    vectorstore.save_local(VECTOR_DB_PATH)\n",
    "    print(\"   üíæ Calculation saved to memory\")\n",
    "    \n",
    "    return {\"answer\": result}\n",
    "\n",
    "print(\"‚úÖ Calculator node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Search Node (Web Information Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(state: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Handle web search queries.\n",
    "    \n",
    "    This node:\n",
    "    1. Performs a web search using DuckDuckGo\n",
    "    2. Uses the LLM to summarize the search results\n",
    "    3. Saves the search and summary to memory\n",
    "    \n",
    "    Args:\n",
    "        state: Dictionary containing the user's query\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'answer' key containing the summarized results\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    print(f\"üîç Search: Searching for: '{query}'\")\n",
    "    \n",
    "    try:\n",
    "        # Perform the web search\n",
    "        print(\"   üåê Querying DuckDuckGo...\")\n",
    "        search_results = search_tool.run(query)\n",
    "        print(f\"   üìÑ Retrieved {len(search_results)} characters of results\")\n",
    "        \n",
    "        # Use the LLM to create a concise summary of the results\n",
    "        summary_prompt = f\"\"\"Summarize the following search results concisely and clearly:\n",
    "\n",
    "{search_results}\n",
    "\n",
    "Provide a clear, informative summary that answers the user's question.\n",
    "Focus on the most relevant and important information.\"\"\"\n",
    "        \n",
    "        print(\"   ü§ñ Generating summary...\")\n",
    "        summary = llm.invoke(summary_prompt)\n",
    "        \n",
    "        # Save the search query and summary to memory\n",
    "        conversation_entry = f\"User: {query}\\nAssistant: {summary.content}\"\n",
    "        vectorstore.add_texts([conversation_entry])\n",
    "        vectorstore.save_local(VECTOR_DB_PATH)\n",
    "        print(\"   üíæ Search results saved to memory\")\n",
    "        \n",
    "        return {\"answer\": summary.content}\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle search errors gracefully\n",
    "        error_msg = f\"I encountered an error while searching: {str(e)}. Please try rephrasing your question.\"\n",
    "        print(f\"   ‚ùå Search error: {e}\")\n",
    "        return {\"answer\": error_msg}\n",
    "\n",
    "print(\"‚úÖ Search node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Step 8: Build the LangGraph Workflow\n",
    "\n",
    "Now we'll connect all the nodes into a state machine that orchestrates the entire agent workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StateGraph with a dictionary state type\n",
    "# The state will store the query and answer as it flows through nodes\n",
    "graph = StateGraph(dict)\n",
    "\n",
    "# Add all our nodes to the graph\n",
    "print(\"üîß Building agent workflow...\")\n",
    "graph.add_node(\"controller\", controller)  # Entry point that routes queries\n",
    "graph.add_node(\"reason\", reasoning)       # Handles conversational queries\n",
    "graph.add_node(\"calc\", calc)              # Handles mathematical calculations\n",
    "graph.add_node(\"search\", search)          # Handles web searches\n",
    "print(\"   ‚úÖ Added 4 nodes: controller, reason, calc, search\")\n",
    "\n",
    "# Set the controller as the entry point\n",
    "# Every query will start here for routing\n",
    "graph.set_entry_point(\"controller\")\n",
    "print(\"   ‚úÖ Set controller as entry point\")\n",
    "\n",
    "# Add conditional edges from the controller\n",
    "# Based on the controller's decision, route to the appropriate node\n",
    "graph.add_conditional_edges(\n",
    "    \"controller\",                    # Source node\n",
    "    lambda x: x[\"next\"],             # Function that returns the next node name\n",
    "    {\n",
    "        \"reason\": \"reason\",          # If next=\"reason\", go to reason node\n",
    "        \"calc\": \"calc\",              # If next=\"calc\", go to calc node\n",
    "        \"search\": \"search\"           # If next=\"search\", go to search node\n",
    "    }\n",
    ")\n",
    "print(\"   ‚úÖ Added conditional routing from controller\")\n",
    "\n",
    "# All specialized nodes lead to END\n",
    "# Once any node completes, the workflow ends\n",
    "graph.add_edge(\"reason\", END)\n",
    "graph.add_edge(\"calc\", END)\n",
    "graph.add_edge(\"search\", END)\n",
    "print(\"   ‚úÖ Connected all nodes to END\")\n",
    "\n",
    "# Compile the graph into a runnable application\n",
    "app = graph.compile()\n",
    "print(\"\\n‚úÖ Agent workflow compiled successfully!\")\n",
    "print(\"\\nüìä Workflow structure:\")\n",
    "print(\"   START ‚Üí controller ‚Üí [reason|calc|search] ‚Üí END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Optional: Visualize the Workflow\n",
    "\n",
    "Let's visualize our agent's workflow graph (requires additional packages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Visualize the graph\n",
    "# Uncomment if you have graphviz installed\n",
    "# from IPython.display import Image, display\n",
    "# try:\n",
    "#     display(Image(app.get_graph().draw_mermaid_png()))\n",
    "# except Exception as e:\n",
    "#     print(f\"Could not visualize graph: {e}\")\n",
    "#     print(\"Install graphviz to see the visualization\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è  To visualize the workflow graph, install graphviz and uncomment the code above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Step 9: Test Individual Components\n",
    "\n",
    "Before running the full chat interface, let's test each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the calculator tool\n",
    "print(\"üß™ Testing Calculator:\")\n",
    "test_calc_result = calculator.invoke(\"25 * 4\")\n",
    "print(f\"   25 * 4 = {test_calc_result}\")\n",
    "\n",
    "# Test the agent with a math query\n",
    "print(\"\\nüß™ Testing Full Agent with Math Query:\")\n",
    "result = app.invoke({\"query\": \"What is 144 divided by 12?\"})\n",
    "print(f\"   Answer: {result['answer']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Component tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Step 10: Interactive Chat Interface\n",
    "\n",
    "Now let's create an interactive chat function to talk with our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    \"\"\"\n",
    "    Run an interactive chat session with the persistent memory agent.\n",
    "    \n",
    "    Features:\n",
    "    - Accepts user input in a loop\n",
    "    - Processes queries through the agent workflow\n",
    "    - Displays responses with formatting\n",
    "    - Saves all conversations to memory\n",
    "    - Exits gracefully on 'exit' or 'quit' commands\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ü§ñ PERSISTENT MEMORY AGENT\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüíæ Memory is automatically saved after each message\")\n",
    "    print(\"üîÑ Your conversations will persist across sessions\")\n",
    "    print(\"\\nType 'exit', 'quit', or 'bye' to end the conversation\")\n",
    "    print(\"Type 'memory' to see memory stats\")\n",
    "    print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        query = input(\"üòä You: \").strip()\n",
    "        \n",
    "        # Check for exit commands\n",
    "        if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(\"üß† All conversations saved to memory\")\n",
    "            print(\"üëã Goodbye! Your agent will remember this conversation.\")\n",
    "            print(\"=\"*60 + \"\\n\")\n",
    "            break\n",
    "        \n",
    "        # Check for memory stats command\n",
    "        if query.lower() == \"memory\":\n",
    "            print(f\"\\nüìä Memory Statistics:\")\n",
    "            print(f\"   Total entries: {vectorstore.index.ntotal}\")\n",
    "            print(f\"   Storage file: {VECTOR_DB_PATH}\")\n",
    "            print()\n",
    "            continue\n",
    "        \n",
    "        # Skip empty inputs\n",
    "        if not query:\n",
    "            continue\n",
    "        \n",
    "        print()  # Add spacing\n",
    "        \n",
    "        # Run the agent\n",
    "        try:\n",
    "            result = app.invoke({\"query\": query})\n",
    "            print(f\"\\nü§ñ AI: {result['answer']}\\n\")\n",
    "            print(\"-\"*60 + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {e}\")\n",
    "            print(\"Please try again with a different query.\\n\")\n",
    "\n",
    "print(\"‚úÖ Chat function defined\")\n",
    "print(\"\\nRun the cell below to start chatting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 11: Start Chatting!\n",
    "\n",
    "Run this cell to start an interactive conversation with your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the interactive chat session\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Step 12: Example Queries to Try\n",
    "\n",
    "Here are some example queries to test different capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries programmatically (without interactive mode)\n",
    "\n",
    "test_queries = [\n",
    "    \"Who is the current president of the United States?\",\n",
    "    \"What is 25 multiplied by 8?\",\n",
    "    \"Tell me about recent developments in AI\",\n",
    "    \"What did we just discuss?\",  # Tests memory\n",
    "]\n",
    "\n",
    "print(\"üß™ Running test queries:\\n\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    result = app.invoke({\"query\": query})\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(\"\\n\" + \"-\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 13: Inspect Memory Contents\n",
    "\n",
    "Let's look at what's stored in our vector memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display memory statistics\n",
    "print(\"üìä Memory Statistics:\")\n",
    "print(f\"   Total conversation entries: {vectorstore.index.ntotal}\")\n",
    "print(f\"   Storage file: {VECTOR_DB_PATH}\")\n",
    "print(f\"   File exists: {os.path.exists(VECTOR_DB_PATH)}\")\n",
    "\n",
    "# Search memory for a specific topic\n",
    "print(\"\\nüîç Sample Memory Search:\")\n",
    "sample_query = \"What have we discussed?\"\n",
    "results = vectorstore.similarity_search(sample_query, k=5)\n",
    "print(f\"   Query: '{sample_query}'\")\n",
    "print(f\"   Found {len(results)} relevant memories:\\n\")\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"   {i}. {doc.page_content[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Step 14: Memory Management Functions\n",
    "\n",
    "Utility functions for managing the vector memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"\n",
    "    Clear all stored memories and start fresh.\n",
    "    \n",
    "    WARNING: This will delete all conversation history!\n",
    "    \"\"\"\n",
    "    global vectorstore\n",
    "    \n",
    "    # Delete the FAISS files if they exist\n",
    "    if os.path.exists(VECTOR_DB_PATH):\n",
    "        os.remove(VECTOR_DB_PATH)\n",
    "    if os.path.exists(f\"{VECTOR_DB_PATH}.pkl\"):\n",
    "        os.remove(f\"{VECTOR_DB_PATH}.pkl\")\n",
    "    \n",
    "    # Create a fresh vector store\n",
    "    vectorstore = FAISS.from_texts(\n",
    "        [\"Memory cleared. Starting fresh.\"], \n",
    "        embeddings\n",
    "    )\n",
    "    \n",
    "    print(\"üßπ Memory cleared successfully\")\n",
    "    print(\"‚ú® Starting with a fresh memory store\")\n",
    "\n",
    "\n",
    "def export_memory_to_text(filename=\"memory_export.txt\"):\n",
    "    \"\"\"\n",
    "    Export all memories to a readable text file.\n",
    "    \n",
    "    Args:\n",
    "        filename: Name of the output file\n",
    "    \"\"\"\n",
    "    # Retrieve all memories (get a large k value)\n",
    "    all_memories = vectorstore.similarity_search(\"\", k=1000)\n",
    "    \n",
    "    # Write to file\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"AGENT MEMORY EXPORT\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        for i, doc in enumerate(all_memories, 1):\n",
    "            f.write(f\"Entry {i}:\\n\")\n",
    "            f.write(doc.page_content)\n",
    "            f.write(\"\\n\" + \"-\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"üìÑ Memory exported to {filename}\")\n",
    "    print(f\"   Total entries: {len(all_memories)}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Memory management functions defined\")\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"   - clear_memory(): Delete all stored memories\")\n",
    "print(\"   - export_memory_to_text(): Export memories to a text file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Step 15: Export Your Memories (Optional)\n",
    "\n",
    "Export all stored conversations to a text file for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to export memories\n",
    "# export_memory_to_text(\"my_agent_memories.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary and Key Concepts\n",
    "\n",
    "### What We Built\n",
    "\n",
    "1. **Persistent Vector Memory**: Using FAISS to store and retrieve conversation embeddings\n",
    "2. **Intelligent Routing**: Controller node that directs queries to specialized handlers\n",
    "3. **Multi-Tool Integration**: Calculator and web search capabilities\n",
    "4. **Context-Aware Responses**: LLM uses retrieved memories for better answers\n",
    "5. **Cross-Session Persistence**: Conversations are saved and loaded between sessions\n",
    "\n",
    "### How Memory Works\n",
    "\n",
    "```\n",
    "User Query ‚Üí Embedding (1536d vector) ‚Üí Similarity Search ‚Üí Top K Memories ‚Üí Context\n",
    "                                                                                  ‚Üì\n",
    "                                                                        LLM with Context\n",
    "                                                                                  ‚Üì\n",
    "Response ‚Üê Save to Memory ‚Üê Extract Answer ‚Üê Generate Response\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Add More Tools**: Email, calendar, database queries\n",
    "2. **Multi-User Support**: Separate memory stores per user\n",
    "3. **Memory Summarization**: Periodically compress old conversations\n",
    "4. **Better Routing**: Use an LLM-based router instead of keyword matching\n",
    "5. **Web Interface**: Deploy with Streamlit or FastAPI\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! üéâ** You've built a sophisticated agent with persistent memory using LangGraph and FAISS.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
